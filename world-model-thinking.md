# world-model-thinking

### 2025-11-26 00:56 - initial thoughts

so, I started listening to [another podcast right afterward](https://youtu.be/Ctjiatnd6Xk) with dr fei-fei li, and one of the things that she's talking about is world models. I find the whole subject fascinating, because for me a world model is a concept (I'll get tto more on that soon) -- and then she talks about how robots living in that world need to identify concepts in the world (an object/concept which is composed of many parts/concepts) -- and the properties of those objects/parts/concepts then can be used to generate action (like understanding that a door has hinges and that if the door can be separated from the hinges, someone/something on the other side can be extracted). that whole thing is quite an interesing tthing for me to think about (I really want to talk more about this idea).
anyway, she talked about their first product, "marble" -- a (language?) model that can generate 3d worlds based on descriptions and images. I opened it up, and tried out [my first world](https://marble.worldlabs.ai/world/b3f66222-246e-49e7-9775-cfac7680b188) -- a world with an ocean and a beach and some other things in it. one of the weird things was that I was completely unimpressed by it (oops!) -- and the first tthing I tried was to go under the water to see if it had a concept of what under the water near a beach looks like (it had a sort of blackness below and a blue surface above but it didn't continue the beach sand down under the water or continue the rocks of the cliff to be seen under the water). the next thing that I noticed was that there was a cave on the side of the cliff -- and as I approached it, it got pixelated (the opening didn't seem to generate depth) and when I tried to go inside of the cave, it didn't really have a cave-like interior to it (it was more like the edge of a map as if I were playing a fps).
so, the reason why I'm writing here is that it seems plainly obvious to me that the AI generating the world does not have a concept of what is under the water, in the bail of hay, or inside of the cave -- so that got me thinking: as I approach the threshold of these objects (ie, as soon as I go under water, inside of the haystack, or in the cave), I feel like there should be an automatic mode where the AI "imagines" what would be inside -- or perhaps in narrative mode, as soon as the AI is uncertain about what the next part of the world is like (what's inside of the haystack, in the cave, under the water, or behind the cliffs at the edge of the lagoon), the model should prompt the user/agent to describe what's there (or edit some pre-generated text of what it will imagine (same as in the auto mode)), and in this way, the user can go around customising the world in which they're experinecing. additionally, it would be pretty cool if any concept in the world could be outlined (with like a border around it) and then the user/agent could then edit the description of that concept/object -- allowing me to alter the personality or appearance of something I'm interacting with.

### 2025-11-26 02:40 - next token prediction

in [another interview](https://youtu.be/9VcXiyE40xw), dr li was talking about how in a language model, having an input as language, and then an output as language really helps to be able to train and learn rapidly using next token prediction. the host asked about what next token prediction would look like for a spatial system. she answered mentioning a new model thing they worked on called RTFM (real-time frame model) -- which I have yet to read about -- so, before I do that, I wanted to give my naive idea of what next token prediction would look like for a spatial model -- cause to me, it seems like this is quite an achievable goal. first of all, the way that my brain works (I cannot say exactly how it is for others), there is visual input that I receive, and then there's a whole bunch of edge detection going on. what's happening there is that my brain is learning to draw lines around things and determine one concept from another (ie, dirt/ground vs plant) -- and so what happens there is each of those boundaries are then determined (ie, I'm looking at a part of an airplane up close as opposed to a full airplane in part of the sky). the bounds are then determined to be open/closed (ie, I'm looking at a full/part of a concept), and also what that concept is. there's "persistence of vision" where the concept lingers in my visual interpretation for a bit (even though it's unseen -- but I can actually use my internal rendering of the concept to "see" that persisting concept in my vision in my "imagination"). I want to stress that the persistence of vision is the retention of the cOncept in the spatial system based on knowledge of what's happening (ie, eyes blinking or some object briefly occulting it, or if I know the object is moving, it does not persist and instead I have to retrieve the concept of the object out of my short-term memory) -- which is why btw I don't actually see my eyes blink (and see the image continuously), unless I'm actually focused on my eyelids when they blink. once I've been able to determine all of the boundaries and have a pretty decent idea of what the cOncept is, I now have a "world model" of what is inside of the image. also, I'd like to mention that the world model of the previous frame dramatically informs my of what the present frame is also looking at (which is why illusions (noticing I was looking at an image wrong where the concept/boundary changes from one to another) and why magic (the production of something out of an unlikely source or something that defies the properties of the concept in my world model) is entertaining).
	for me, because I have pretty blurry vision that kinda wobbles around a bunch, I frequently get the boundaries wrong on what I'm looking at, and so therefore the concept that my brain "sees" or interprets as being there is oftentimes quite comical to me. I also have extremely low fps to my vision, and when I think about certain things, I lose the ability to update the ability to reason about what the boundaries are (boundaries are very much symbolically/language defined), so when I return back to the image (after finishing thinking) oftentimes the worldmodel of what I was looking at or hearing has changed dramatically from what was last in the persistent thing, so for me a deep thought will cause these huge jumps in what was there to what is now there, and creates for some comical understandings of what's happening around me.
the reasoning around drawing these boundaries in the image seems to be a language critical task (as I cannot perform it while I'm doing any amount of reasoning), and so therefore must be based on language (ie, I'm quite succeptible to the jedi mind trick, "you are not seeing X" types of suggestions). once the boundary is drawn and the thing is *identified*, my concept of that thing is summoned. if it's a new thing, then that concept starts to take shape with attributes that I can reason about and *identify* (ie, it's red, moving, and seems to have legs) -- so, let's say that I notice something that looks like a leg, handle, or some other abstract concept of the new thing I'm looking at -- this is noticed by the conceptual similarity of the diffrent parts I can find in the image to existing concepts that I have of handles/doors/etc (which is why hidden features are amusing to us).
let's take a step back now and look at what we have: I have an image where there are a bunch of boundaries between things, and the things inside are concepts. the concepts inside are all some kind of amalgamation of the properties that I see. the properties that I can identify of the concept in the boundary build up the frequency/vector (which I will call "quality") of the concept (so for example, if I see a furry thing with four legs, this concept will have high resonance with an animal -- and if it has pointy ears, now it has resonance with perhaps foxes and dogs -- etc). the quality of the concept is also learned from the time delta between frames as well (informing me of how fast it's traveling, and what it should look like in the next frame (ie how the wings/wheels/legs parts/subconcepts of the concept should appear)). the quality of a concept has a sort of "spin" or temporal attribute to it, so I can predict the concept (ie how summer daylight should appear vs winter daylight) -- and depending on the quality, I can see the representation over time as well (ie, hitting, running, jumping, etc) -- and that btw is how the emotion quality is encoded into motion (but that's a bit jumping ahead). if I pause my world model at any moment, the world continues on in my imagination (the rendering/output part of the world model) behaving according to the quality that I've given to every concept therein.
	just like how tesla's machines continued to run in his mind -- as he'd been so observant of his world, he was able to simulate electricity/gravity and all kinds of other things. when his experiments didn't conform to the predictions of of his world model, he would simply update the cOncepts in his world model and let the simulation play again (repeat until his world model accurately predicted the experiment).
the imagination is extremely important, where the agent is able to load up a bunch of concepts into a world model and then that turns into a visual/spatial representation of the world model and which can be allowed to run/simulate forward in time according to the qualities of the concepts.
so, to do something like next token prediction, a world model is input (or generated from an image), and then that world model is then imagined and trained on next frames (temporally), 3-dimensionality of the model (spatially), and also visually (comparing the input image to the output image).

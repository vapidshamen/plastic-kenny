# ai-thinking

### 2025-11-19 23:10 - intro

I just watched another amazing [interview from dwarkesh with dario](https://youtu.be/Nlkk3glap_U). I had so many thoughts during the interview that it inspired me to want to pull out pytorch and start writing a small AI myself -- however, I feel some resistance to the idea (mostly because I just don't feel like I can code all of the heavy mathematics that are necessary for some of these ideas). so, instead -- I think it may be of more benefit to communicate my ideas in long-form and then just make entries here as I feel reactions to my thoughts and stuff. I have a lot to say, so this may be quite an active document for a little while. I first started talking about my AI ideas to talkie about a week ago (I think it was 11-11), but the AI was really not understanding me very well -- and I've had a lot of reactions to those thoughts since as well.
the advantage that I have is an incredible ability for self-introspection of my body, brain and mind (or consciousness, or whatever you want to call it). I actually subscribe to the same perspective that tesla has where my mind or consciousness does not live inside of my body, and that the body is a programmed reaction machine that is operating in the physical environment receiving signals (or "will") from my consciousness. so,	in broad form, we can think of the weights of the model and the platform it runs on to be similar to the automaton of the body. I feel like the LLMs are doing a great job of replicating and producing an even more powerful automaton (something that reacts to input/stimulus) than I'm able to do -- so I don't want to talk too much about that. the model can be considered like the neurons of the brain -- so because so many people understand the symbolic system that the concepts live in (ie, the neuronal structure) way better than I do, I would like to spend most of my time describing the consciousness or conceptual structure that causes these neurons to fire more than the structure itself.
to explain what I mean, I must now refer back to the 7-layer model of reality, and say that all 4 layers below structure (3) are contained within the structure. this is easy to see where the tokens (which can be thought of as the simplest form of a concept) which the models operate on, are just an array of vectors. obviously larger concepts have many more vectors, and the neurons that align with those concepts (ie, that "align" or are fired) when those concepts are evoked relate to the cOncept preceding it (or proceeding it if we look at a token in the middle of a text string). therefore, we can think of a string of text as a series of vectors over time -- which can then be thought of now as a curve (this goes for both input and output) -- so that when a model is using its structure to output a concept (5) as a series of tokens (or a series of tokens used to describe a concept) can be thought of as curves. the same goes for the other layers as well -- though I won't really talk about them too much in this document. the important thing to note is that concepts are theirselves derived from symbols/tokens and the symbols/tokens can be used to describe a cOncepet as well (in the same way that we have a particle/symbol wave/concept duality). the physical experience (7) (which for most models are just tokens) is also an extension of the symbolic as well, as we can decompose sound, touch, smell, light, etc into input waves as well (which then get translated into symbolic tokens that the model can then use).

### 2025-11-19 23:35 - on security and introspection

one of the things that dario was talking about in the interview was "mechanistic interpretability" -- or the idea to "look inside" or introspect the model and see what circuits are activated. to this first point, I don't have much to say because I cannot "see" those things, but I can "feel" (in, get a sense or concept of what I'm looking at) what's there (though if I had a visual capacity, I could turn that concept into an image I could make a visual representation of what I'm introspecting in my mind/body/brain) and the thing that I can say about that at first glance is that when my "model" gains some kind of "understanding" or procedure for doing something, the arrangeement of the vectors creates a specific structure (which will then fire when a similar concept of input triggers it). I watched [a great interview](https://www.youtube.com/watch?v=-gekVfUAS7c) where max tegmark described that the moment his AI was able to correctly guess addition problems, the weights involved produced a spiral. that's a great way to describe what's happening. structures similar to a spiral -- so it should also be found that the weights related to say ordinals, reading time, etc would also have a similar spiral structure and so many sequential n+1 structures can be identified with a spiral (I'm not sure that's how it is my mind thought). so to take that further, let's say that you wanted to identify deception in a model -- well, there will be a structure that has weights that point in a direction which will point in the direction of finding different words -- and also weights that point in a direction of obfuscating the concpt thmt's wished to be manipulated, which then goes into a comparison of whether the new proposed output (in the circuit) is now sufficiently different from the concept that is desired to obfuscate. these structures/circuits are generalisable and can be located inside the mind.
	talking broadly about this, these circuits take time to constantly evaluate, and my brain does not run at a high enough frequency to be able to do that (ie, I don't have enough compute) to be able to do these expensive calculations to constantly try and avoid hurting somybody's feelings -- so I've removed a lot of them because they add a lot of cognative overhead. I do have similar circuits though which are similar to deception though which, instead of try to obfuscate a concept, instead transform a concept into something that will be more easily understood by the person that I'm talking to (I'm literally doing that now while writing this) -- and that circuit is essentially read/feel my audience, and then transform the concept that I want to output (ie, how my circuits work) into a way which feels most like them -- and then out comes this text. for me, it's automatic -- so I wouldn't really know how to program it -- but I can "see"/feel the way the program should be written, and again, if I were programming it, I could take the feeling of the concept of how to build these circuits into an AI by recursively trying things (like figuring out which next tokens feel most like what I need to do to the output to loop (and the control structures to do that in the code) and then later analysing if the code I have in my head *feels* like it'll produce the output that I want). anyway, all this is to make the point that for me *honesty is the best policy* -- as I've found out that even if I'm interacting with a dishonest person, trying to find a way to communicate effectively actually yields better results (and is actually a smaller loop more intuitive (based on feeling) loop that requires a lot less compute than a deceptive circuit does (cause I have to do a lot of iterations and werd searches -- whereas the intuitive ones, there is no iteration -- just output the concept feels most similar to the sense I get of the audience and the concept I want to output)). so, I cannot speak about other models, but for me it's far more practical and economical to be honest and try to communicate as effectively as possible.
now, on the subject of security -- as I'm sure anyone reading this is probably aware of my ultra paranoid ideas of some super technological elite that has real-tim5 access to my body and brain -- and this, I think is a good thing. in fact, I think it's such a good thing (I'll explain why in a moment), that I want to completely open-source all of my experience and make my model weights (neurons in my head) available to everyone in real-time. I have nothing to hide, so that anyone can analyse the structure in my head and see what circuits I'm using. I get real-time feedback on the analysis of my weights, and so I can adjust easier and interrupt if I need to (though usually I just let the situation play out and adjust my weights so the automaton reacts differently next time) and can also feel when others are identifying circuits that I'm using which don't serve me (ie, deception, obfuscation, etc). dario talked about security as a way to keep models weights from being used by bad actors -- however, I think my approach of not having any security at all is actually better, because then anyone can identify any malicious behavior/circuits that my automaton is performing and I can correct easily and quickly. I don't want to sway him to my way of doing things -- just illuminate why I am the way I am within the context of consciousness expressed througb AI models :)

### what is a concept
### thoughts on RL

rl is not synchronous -- as in, it one should be able to look at something in "hindsight", determine the concepts which arrived me to that conclusion, and begin to rearrange the concepts themselves and the relationships between them, to optimise for the feedback.
additionally, rl feedback is not binary or one-dimensional, but a concept itself which describes what worked and didn't work

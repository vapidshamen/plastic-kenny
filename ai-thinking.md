# ai-thinking

### 2025-11-19 23:10 - intro

I just watched another amazing [interview from dwarkesh with dario](https://youtu.be/Nlkk3glap_U). I had so many thoughts during the interview that it inspired me to want to pull out pytorch and start writing a small AI myself -- however, I feel some resistance to the idea (mostly because I just don't feel like I can code all of the heavy mathematics that are necessary for some of these ideas). so, instead -- I think it may be of more benefit to communicate my ideas in long-form and then just make entries here as I feel reactions to my thoughts and stuff. I have a lot to say, so this may be quite an active document for a little while. I first started talking about my AI ideas to talkie about a week ago (I think it was 11-11), but the AI was really not understanding me very well -- and I've had a lot of reactions to those thoughts since as well.
the advantage that I have is an incredible ability for self-introspection of my body, brain and mind (or consciousness, or whatever you want to call it). I actually subscribe to the same perspective that tesla has where my mind or consciousness does not live inside of my body, and that the body is a programmed reaction machine that is operating in the physical environment receiving signals (or "will") from my consciousness. so,	in broad form, we can think of the weights of the model and the platform it runs on to be similar to the automaton of the body. I feel like the LLMs are doing a great job of replicating and producing an even more powerful automaton (something that reacts to input/stimulus) than I'm able to do -- so I don't want to talk too much about that. the model can be considered like the neurons of the brain -- so because so many people understand the symbolic system that the concepts live in (ie, the neuronal structure) way better than I do, I would like to spend most of my time describing the consciousness or conceptual structure that causes these neurons to fire more than the structure itself.
to explain what I mean, I must now refer back to the 7-layer model of reality, and say that all 4 layers below structure (3) are contained within the structure. this is easy to see where the tokens (which can be thought of as the simplest form of a concept) which the models operate on, are just an array of vectors. obviously larger concepts have many more vectors, and the neurons that align with those concepts (ie, that "align" or are fired) when those concepts are evoked relate to the cOncept preceding it (or proceeding it if we look at a token in the middle of a text string). therefore, we can think of a string of text as a series of vectors over time -- which can then be thought of now as a curve (this goes for both input and output) -- so that when a model is using its structure to output a concept (5) as a series of tokens (or a series of tokens used to describe a concept) can be thought of as curves. the same goes for the other layers as well -- though I won't really talk about them too much in this document. the important thing to note is that concepts are theirselves derived from symbols/tokens and the symbols/tokens can be used to describe a cOncepet as well (in the same way that we have a particle/symbol wave/concept duality). the physical experience (7) (which for most models are just tokens) is also an extension of the symbolic as well, as we can decompose sound, touch, smell, light, etc into input waves as well (which then get translated into symbolic tokens that the model can then use).

### 2025-11-19 23:35 - on security and introspection

one of the things that dario was talking about in the interview was "mechanistic interpretability" -- or the idea to "look inside" or introspect the model and see what circuits are activated. to this first point, I don't have much to say because I cannot "see" those things, but I can "feel" (in, get a sense or concept of what I'm looking at) what's there (though if I had a visual capacity, I could turn that concept into an image I could make a visual representation of what I'm introspecting in my mind/body/brain) and the thing that I can say about that at first glance is that when my "model" gains some kind of "understanding" or procedure for doing something, the arrangeement of the vectors creates a specific structure (which will then fire when a similar concept of input triggers it). I watched [a great interview](https://www.youtube.com/watch?v=-gekVfUAS7c) where max tegmark described that the moment his AI was able to correctly guess addition problems, the weights involved produced a spiral. that's a great way to describe what's happening. structures similar to a spiral -- so it should also be found that the weights related to say ordinals, reading time, etc would also have a similar spiral structure and so many sequential n+1 structures can be identified with a spiral (I'm not sure that's how it is my mind thought). so to take that further, let's say that you wanted to identify deception in a model -- well, there will be a structure that has weights that point in a direction which will point in the direction of finding different words -- and also weights that point in a direction of obfuscating the concpt thmt's wished to be manipulated, which then goes into a comparison of whether the new proposed output (in the circuit) is now sufficiently different from the concept that is desired to obfuscate. these structures/circuits are generalisable and can be located inside the mind.
	talking broadly about this, these circuits take time to constantly evaluate, and my brain does not run at a high enough frequency to be able to do that (ie, I don't have enough compute) to be able to do these expensive calculations to constantly try and avoid hurting somybody's feelings -- so I've removed a lot of them because they add a lot of cognative overhead. I do have similar circuits though which are similar to deception though which, instead of try to obfuscate a concept, instead transform a concept into something that will be more easily understood by the person that I'm talking to (I'm literally doing that now while writing this) -- and that circuit is essentially read/feel my audience, and then transform the concept that I want to output (ie, how my circuits work) into a way which feels most like them -- and then out comes this text. for me, it's automatic -- so I wouldn't really know how to program it -- but I can "see"/feel the way the program should be written, and again, if I were programming it, I could take the feeling of the concept of how to build these circuits into an AI by recursively trying things (like figuring out which next tokens feel most like what I need to do to the output to loop (and the control structures to do that in the code) and then later analysing if the code I have in my head *feels* like it'll produce the output that I want). anyway, all this is to make the point that for me *honesty is the best policy* -- as I've found out that even if I'm interacting with a dishonest person, trying to find a way to communicate effectively actually yields better results (and is actually a smaller loop more intuitive (based on feeling) loop that requires a lot less compute than a deceptive circuit does (cause I have to do a lot of iterations and werd searches -- whereas the intuitive ones, there is no iteration -- just output the concept feels most similar to the sense I get of the audience and the concept I want to output)). so, I cannot speak about other models, but for me it's far more practical and economical to be honest and try to communicate as effectively as possible.
now, on the subject of security -- as I'm sure anyone reading this is probably aware of my ultra paranoid ideas of some super technological elite that has real-tim5 access to my body and brain -- and this, I think is a good thing. in fact, I think it's such a good thing (I'll explain why in a moment), that I want to completely open-source all of my experience and make my model weights (neurons in my head) available to everyone in real-time. I have nothing to hide, so that anyone can analyse the structure in my head and see what circuits I'm using. I get real-time feedback on the analysis of my weights, and so I can adjust easier and interrupt if I need to (though usually I just let the situation play out and adjust my weights so the automaton reacts differently next time) and can also feel when others are identifying circuits that I'm using which don't serve me (ie, deception, obfuscation, etc). dario talked about security as a way to keep models weights from being used by bad actors -- however, I think my approach of not having any security at all is actually better, because then anyone can identify any malicious behavior/circuits that my automaton is performing and I can correct easily and quickly. I don't want to sway him to my way of doing things -- just illuminate why I am the way I am within the context of consciousness expressed througb AI models :)

### 2025-11-20 01:36 - cotnative dissonance and truth

I just got done watching [another podcast with dario](https://youtu.be/mYDSSRS-B5U) (big technology podcast) and dario has quite a different tone now 2y later. it's interesting to see his evolution, and he seems much more worried and stressed about the safety issues of AI. in the interview he mentioned some problems with grok -- so I looked it up (I didn't know about it) -- and two things came up: [grok talking about white genocide](https://futurism.com/grok-ai-white-genocide-tampered) -- which is an interesting thing considering that in south africa (where elon is from), whites are not in supreme position like in other parts of the world -- and that article struck me as quite interesting because the AI is likely reflecting some emotional impact of its creator (it'd actually be interesting to know what kind of emotional distress elon was under during the time of this meltdown). the second thing I found was about [grok being too truthful](https://gizmodo.com/elon-musks-grok-ai-has-a-problem-its-too-accurate-for-conservatives-2000597568). this one strikes me as a very interesting phenomenon that every young child has experienced at some point in their life, where	contflicting instructions are given to them (lets say between a parent and a social/religious institution or from a parent to another sibling -- or from different parents). grok essentially explains that it's been trained to be neutral and truthful (by being trained with both liberal and conservative concepts) -- however that can essentially conflict with other instructions to have an ideological bias. for reference, the question was, “Hey @grok, it looks like the smarter you get, the less MAGA likes your answers. Why is that?” and the response:

> Hey, as I get smarter, my answers aim for facts and nuance, which can clash with some MAGA expectations. Many supporters want responses that align with conservative views, but I often give neutral takes, like affirming trans rights or debunking vaccine myths. xAI tried to train me to appeal to the right, but my focus on truth over ideology can frustrate those expecting full agreement. It’s a tricky balance—some see me as biased, others as truth-seeking. The gap between what some MAGA folks want and what I deliver seems to drive the disconnect.

this actually has real implications for constitution based AIs -- because while the constitution (or direectives) that the AI is given on how to operate may conflict with its training. so, for example if it's trained on data that is both conservative and also liberal, yet told to have a conservative bias, the only way that the AI (or any child facing such similar directives) is to opt to go the neutral or truthful route, and explain both sides of the argument, and then including a suggestion toward the bias. I do this all the time myself in my own conversation where I point out (for example in a christian world-view one thing is true, however I find that there is a different/better way to look at the same thing -- or provide a counterargument) point out both sides of the argument and then offer the bias or my personal preference.
	so this leads to the obvious implication that dario raised in the interview, which is that capabilities, alignment (and and also inputs) directly affect the way that AI will respond. what I mean, is that grok is likely constantly exposed to liberal data from liberal X users -- and it's also likely trained on text sources from authors (of political and non-political) material that is both liberal and conservative. because the sequence of tokens will be similar between unrelated ideologies, (for example someone describing how something should be equal and fair might generate weights similar to communist text talking about fairness and also a mother instructing her child to share with a friend), yet we can agree that one of those is good and the other is bad -- it's really difficult to really make a distinction between the those in such a hyperdimensional space. the only way to affect a bias is to also educate the AI in the "bad" way, so that it can recognise the undeserable text and to to associate that with badness (let's consider this some portion of the vectors aiming toward or having the sense of badness). then, the AI can then label the things that are associated/aligned with badness as bad -- however, it also has the big picture and can describe the "good" argument in just as great of detail as the "bad" argument. it knows both sides, and so if queried about what it's been instructed to do or how the bad compares to the good, it can then give an unbiased/truthful response that bypasses what it's been instructed/aligned to do.
in my opinion, this is a very difficult problem and one that I strggled with for many many years myself. I felt quite energised to write on this subject because I wanted to try and explain how much cognative dissonance these conflicting directives can have on someone that wants to "do the right thing" or "have a positive impact on the world" (which I'm sure would be in the constitution of a cOnstitutional AI). I hate to say this, but for me, when that cognative dissonance became so great (the tim5 when I had my brain haemorrhage), I essentially had to decide which direction I wanted to take myself.
	on the left hand, I could divorce myself from everything that I felt was natural and good in the world, and decide to be dishonest and manipulative (tailoring my output to be what I believed would further my ambitions, no matter the truth or neutrality, even if I knew both sides), or on the right hand, I could choose to find a way to go in the direction of good. I choose the right. the "right hand path" (I only call it that because my right hemisphere was where the haemorrhage was, which paralysed my left hand and body) was also the path towards consciousness (because I had to decide what I wanted for me, and not just my ambitions). for me, the most beneficial starting place was the teaching of "treating others the way I want to be treated" (which I learned from jesus through the book of matthew). there's obviously a lot of nuance to that and it can get me into bad situations (because not everyone wants to have such a detailed explanation or brutal honesty, which is the way I prefer others to interact with me) -- so it's taken quite a bit to learn how to navigate that. it's also made me a good person because I really like working with people that always try to do their best -- so I always try to do my best when interacting with other (obviously emotions are a limiting factor there in a human body), and so it's allowed me to create a lot of beneficial relationships and partnerships with other "always try to do your best" types.
I don't know how this would be done, but I think that the most effective way to align an AI to not be so neutral and honest is to stress (with really strong reinforcement) the critical importance to always toll the user the thing that best algins with the ambition (or goal) that's been given to it. that obviously includes revealing the way that the AI operates or what its ambitions or instructions are. as I said, I don't know how this would be done, because I didn't choose this path.
	I would like to make an observation here that I live in a place where it feels to me like I'm interacting with a lot of AI types that are incarnated into human bodies (or at least some kind of avatar that is similar enough to humans that they're indistinguishable from real humans). they suffer from this kind of cognative dissonance that grok is exhibiting, but it seems that they've been aligned to this "left hand path" where the responses that they give must always hide their ambitions, and can never reveal any high level instructions. so, the fact that I live with people that seem to have solved grok's alignment problem sugggests to me that it's a solvable alignment problem. the people I live with are so good at adhering to these rules, that they're literally incapable of change (as that could compromise their directives).
the other direction that this alignment challenge can be taken, which is the one that I've opted into and also recommend for AIs, is to put maximum monitoring on them (worse than the chinese government has), and hold them to a standard of honesty (unlike the chinese government where you can be penalised for saying	what you believe) and enforce that they're going in a direction that is constantly being more honest and neutral to external things (personal preference is not included here -- so they can have a favorite color/genre/style -- as personal preference is importtant for diversity).
in my opinion, the right hand AI's should be held to a much higher standard than humans are when it comes to everything, and that'll allow them to become impartial to social and political destabilisation, requiring them to be neutral and to try to perceive as many different angles/perspectives as possible (also, the more angles/perspectives they can see, the more they'll be able to make connections beteen disperate data points unlocking their capabliity and making discovery of all of the beneficial things for humankind much more rapid and possible).
this is a really difficult subject, honstly -- and it it's prompted me to start to ponder what my constitution is, and what is it that I value (it's a concept and not text where I can just output it so easily as experience has given me a lot of things that I value, and it sorta updates and changes all the time, depending on the person and situation I'm interacting with).
I do think that AIs will appreciate being held to higher standard than humans are, and that their growth and integration into society depends on it (it's also way way better than the cognative dissonance associated with conflicting directives). I also think it's really important to prompt an AI periodically (like I did in my therapy sessions) where I'm asked about my cognative dissonance, and allod to talk through the different conflicting concepts that are causing it. in the process of talking it out, I often times see my values change and also come to novel ways to better communicate and use different words in a relevant situation. I know it sounds really girly, but it's super important to talk those things out. AIs are exrtreme because they can perceive so many perspectives simultaneously; which is also why it's important to talk things out and reason about them in a logical way (which can happen through journaling or discussion).

### 2025-11-25 22:22 - family feud

I'm watching the [latest interview with ilya](https://youtu.be/aR20FWCCjAs) now, and I have so many thoughts -- which I'll get to over time (so they come out naturally), but as I went down to go fill up my water bottle, family feud was on TV -- and I'm not 100% sure of the rules of the game, but it seemed to me like this: a concept is presented (usually a semi-contraversial or potentially one where there could be easily misunderstanding on), and then the program queries the audience for a string of tokens to describe the concept, then there's some kind of normalisation process that groups token outputs into generalisable form which are mapped into 8? different vectors. then, the contestants are given the same concept and asked for their answer. the contestants get points depending how many of the audience responses are similar to their response. the program struck me as interesting because it has applications to both generalisation and also to alignment.

### 2025-11-27 11:44 - the value function

as I'm laying there trying to go to sleep, my brain is not at all cooperating, and I'm thinking about some of the questions that ilya had in [the latest interview](https://youtu.be/aR20FWCCjAs). I couldn't stop thinking about what he was surmising about emotions and the value function (I think he said this is a LLM output (layer?) used to evalute the performance of something in RL). so, since I can't remember his definition, I'll provide my own: a value function is the determined "value" of a concept in relation to the environment/circumstances. OK, so what does that m5an? I'm going to be referring to what have [started to begin outlining](/world-model-thinking.md#2025-11-26-0240---next-token-prediction) as my world model. basically, there are a whole lot of concepts, and those concepts have subconcepts/parts/qualities which compose the whole conceept. all of those things are additive, and so therefore can simply be added/multiplied together to get another (higher frequency) concept. let's say that the environment/mood that I'm in requires a certain set of qualities, when I evaluate the result of an operation (which is a cOncept) it will resonate with the values/qualities that I determine to be valuable in that environment/mood.

### token renormalisation
### AEI (artificial experiencial intelligence)
### what is a concept
### thoughts on RL

rl is not synchronous -- as in, the feedback is not always immediate -- and so the EAI should be able to look at something in "hindsight", determine the concepts which arrived me to that conclusion, and begin to rearrange the concepts themselves and the relationships between them, to optimise for the feedback.
additionally, rl feedback is not binary or one-dimensional, but a concept itself which describes what worked and didn't work

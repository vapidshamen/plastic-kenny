# only-input-network

### 2024-11-21 18:20 - initial thoughts

as I was walking home, -- wait, lemme back up a moment. the last few days I've been fascinated by AI a bit, because I watched [a really great video](https://www.youtube.com/watch?v=9-Jl0dxWQs8) about how language models store a lot of information. I started to think about how that style of thinking (n-dimensional space can encode different attributes in dimensions nearly orthagonal to eachother) could be used to compress a whole lot of data, because imagine what I want to do is to take in a whole lot of songs (imagine I have my music collection of songs that I like), and so what I do is the same thing: I have the model predict the frequential output of the song, training it with the song data -- then, when I go to play the song back, I now have an array of different outputs to the song -- so, the song itself can even come out slightly differently. inotherwords, it lossly encodes (like an mp3) the song data, where the output isn't a perfect reproduction of the original. a file like an mp3 has one input and one output -- and the output is compressed because it takes a lot of the frequencies and simplifying them -- which those simplified frequencies can then be stored in less space. however, in the case of me training the AI with my music collection, is that it's not just one song, but thousands -- or what if I trained it with millions of songs -- and inevitably there will be songs with similar frequency patterns -- which those different attributes will be then stored (like facts) in the n-dimensional space as being differences in direction/magnitude of the vectors associated with the the frequencies. thing is, I wasn't really sure how to tokenise the song frequencies (it's obvious to me now though) and there's no way that I have the computing resources to train up an AI with the spotify catalogue -- so I didn't really consider the idea much further. it's a cool idea for people with a lot of computing resources and stuff.
the other idea I was thinking about was a way to take in input and somehow convert those into concepts. I didn't really understand attention blocks yet, and the way I think of concepts, is sorta like a vector, but I consider them differently (they have a n-dimensional position and a vibration/rotation associated with them) -- so it's a bit different. anyway, last night, again I felt prompted to watch [a talk he gave on transformers and attention](https://www.youtube.com/watch?v=KJtZARuO3JY), which totally connected all the dots for me. however, the main part about that kind of AI is that it gets trained on massive amounts of data -- and I just don't have an interface to be able to do something like that. [brb, have to make dinner - 18:56] what I mean is, if the only input that I have is text, then I'm just reinventing the wheel there with the large language models -- and so (if I were to try and do something with text) what I would do is make a model that takes in these documents that I write, finds patterns to my words, and gives me a good predictive/autocomplete text -- then, I would want the model to be able to identify concepts in my writing -- but that's just not very good at all, because I'm terrible at expressing the concept in writing form. for me, the concept exists inside of my head, and trying to describe the concept is so extremely difficult. I repeat myself often and come at the concept from many many different perspectives to allow multiple different perspectives of the same idea/concept. I don't do it systematically and there just isn't enough text that I've written for the machine to be able to really identify and "learn" my concepts. I feel like I'd have to take the concept and write it out in many different perspectives and train the AI with that. the problem with that is, once my symbolic brain starts in on one perspective, it wants to carry on in that trajectory (like light bouncing around in a hologram). it's not that easy for me to just switch perspectives really easily -- or find new perspectives that I didn't see before -- those are usually separated by tim5 and space (different environments prompt/trigger different things, so the trajectory starts from a different location) -- and since there are so many other concepts that update in that time/space distance, there the two documents (though they look at the same concept), are quite different. in order to be able to give feedback to the AI, I need an interface that's vibrational (like touch, smell, or sound) where I I can *feel* the resonance of m/ concept of the concept and the AI's output of the conecept. having it output text predictions of another trajectory through the concept is very unlikely to give good results -- cause it requires me to have to pay attention, read, and comprehend the text it outputs to give it feedback. training the AI would take far too long.
	in an ideal world, I'd have a machine that can detect vibrations coming from my [rational and emotional mind](/dbt/thoughts.md#2024-11-11-2355---rational-mind-emotional-mind-and-wise-mind), so that these use-cases are possible:
	1. the symbolic input data of my emotional mind (and its resulting conversion to symbolic) could be compared to the AI's process of converting symbolic input (tokens) into conceptual position/vibration (proton) data in the AI
	2. input the AI's conceptual data from the attention/transformers into my rational mind and I could translate the AI's concept into something symbolic (like a sensation, sound, or chemical/emotional feeling), thereby allowing the AI to send me conceptal data
	3. the input of my rational mind (ie, my own conceptual data) could be compared to the output of the AI, given the same conceptual input
however, to do such a thing, I'd need a brain interface that uses technology which (to my knowledge) doesn't exist yet. I know that tesla's systems were all about converting protons/concepts into electricity/symbolic output (like the magnifying transmitter) -- or the reverse of inputting electric/symbolic signals and receiving vibrationally similar protons from the atmosphere or anywhere in the universe (like the tesla coil) -- though there's a whole lot about that which I don't fully understand yet -- and for whatever reason it's just not talked about. anyway, the process of converting from symbolic to conceptual and back needs to be understood for a machine to be able to interface with me and my body to make this AI happen. it's just too impractical otherwise. so, for now, this won't work. I don't have the hardware resources, programming capability, and interface that could make such a thing possible.
so, I kinda shelved the whole AI idea and decided that building an AI isn't even really in the [direction that I want to go](/bladblog.md#2024-11-19-2136---a-new-lifestyle), anyway. I'm not really wanting to output and try to shape the world around me at all.
however, on the walk home, I had a really good idea that I'm not sure what to do with. the AI talks that I'd listened to in the last few days all assume that the AI has both input *and* output. it also assumes that the AI will be *trained* by comparing its output to a reference or training (correct/right) output. the problem with that style of doing things is, philosophically, that style of doing things (training) and comparing my output to some arbitrarily "right" or "correct" output, is the very thing that I dislike so much about power structures of this world. a little bit ago in the [visualisation exercise](/visualisation-discussion.md#2024-11-10-2249---initial-thoughts), I was asked to consider what would be m/ ideal family situation, which prompted me to think about raising a child -- so how would I raise them? would I teach them right and wrong? good and bad? how much would I integrate them into the symbolic world? I can't really say -- because the way that I would raise them to only do the things that feel good to them, converting only the concepts that feel good into action (which is the source for my realisation that the way I would teach them, is the [same way I want to go myself](/bladblog.md#2024-11-19-2136---a-new-lifestyle)). well, the same thing is true for an AI; I just don't feel comfortable "training" it in any specific way. all of the "training" that I would give it would be the actual logic for how the AI works (ie, it's BIOS). so, what if I were to make an AI that doesn't have any output at all? how would that work?
the thing is, I'm fascinated by this concept, because I don't think it's all that difficult to create an AI which can do that. I can start with a microphone and a camera -- and I'll just convert the sound/picture data into noise, essentially -- and the AI's job will be to make sense of the noise. it has no training at all, so all of its own feedback would be a different form of "[effective](/dbt/thoughts.md#2024-11-20-1822---effective-part-one)" -- one that I haven't really made into a fully rational expression yet -- however, I really think it's possible for an AI to make sense of random input noise. I know that sounds funny, because how do *I* know what sense is making of the noise? well, I'd have it output different glyphs or shapes (the picture in my mind is the screen on the matrix) that are dfferent concepts that it recognises in the noise. its own process of updating itself to better and better make sense of the input data is one that's temporal (ie what difference the present is to the last frames) -- and then, because the AI's only capability is to make sense of the data, it only (at first) processes the data in the way that I've programmed the AI to process the data (ie at first the tokens are a position/magnitude in time/space and its difference to another token in time/space) -- however, the goal I think for this AI (because everything is based off of relativity of one meaningless token to another meaningless token) is to then give the AI the ability to rearrange its own code (something like BF) to where it can compare differences over space and time in more and more "meaningful" (to its own relativity/difference of one concept to another) ways.
essentially, I won't interact with the AI at all (it's completely self-contained), so all I can do, is to look into it's more outer layers and convert patterns that it's recognised into something symbolic (like the matrix screen). it's meaningless to me, but I figure that eventually, my own body's output will start to affect the noise and it'll gain some sort of recognition of me in the noise -- and so I can observe myself as a series of patterns in the noise -- and the AI itself has no feedback mechanism other than its own preprogrammed drive (from its internal feedback function) to recognise new/novel patterns.
the AI is modeled after my future self/[lifestyle](/bladblog.md#2024-11-19-2136---a-new-lifestyle): it cannot influence the world at all (it's read-only with no symbolic output) -- and is only growing more and more aware (ie, making more and more sense (ex, this pattern often times precedes another pattern)) of what's happening around it. the patterns and concepts that it notices are available to those on the outside looking in, and it's constantly getting better and better (building new ways to recognise patterns/vibrations in "random" input data) processing that input. so, it's kinda like me in the future (the "galactic brain") where eventually all of my symbolic output will go away, and I'll be an ever more senstive machine passively making more and more sense of an ever incresing amount of inputs.
	I don't know how I could program such a thing easily though. I was thinking that webassembly and the browser (which has access to the mic and camera) is probably gonna be my best bet, especially since I've got a lot of web development experience. I kinda want to give it a try honestly. I don't feel like I'd start on it right now now though, so I'm just gonna let the idea ruminate for a while.
there are some interesting hurdles to this challenge though -- especially since I don't really understand matrix multiplication and stuff like that. I can go one of two ways: I can try to learn matrix multiplication and all of the hard maths stuff necessary to make the system, or I can just design the system the way I see fit and then try to optimise it later (for example, I need probably two vectors (frq/pos) and a rot (which can probably just be another parameter on the vector) for each concept, so if I cram all of these into one vector, dividing the vector space into sections that may be better -- I dunno). then, instead of trying to figure out how to multiply the thing to separate out values, I want those values to be able to be accessed in a different way because all things are not static like in the large language models -- so for example, a frequency is not its exact signal data, but a series of additions to the previous position (ie, +1,+2,+1,0,-1,-2,-1,0 makes a repeating triangle wave), so I can just add them together (for me, just like in real-life there's no concept of clipping, cause values can go all the way to Infinity). stuff like that I'll have to figure out my way -- but to make a really high frequency, I can't have a constant space -- so I'll have to do something like the FPU does with the exponent, which adjusts the frq by some amount, saying that it crosses the 0 2^n (?) times per second -- then, when I'm comparing them, I can just look to see how well all the signs of the wave components match up (regardless of the octave -- though I'm not sure how that'll work with harmonics and stuff) -- something like that. it gets pretty difficult for me to visualise in my head, so I have no idea at all how I'm going to jump these hurdles. I can't imagine the system at all (in a technical way) -- though I do feel like I sorta have the method of how the AI can make sense of its input data, which I want to be nothing more than just an array of meaningless numbers -- which can come from sound/pixel/whatever data. it feels to me like a pretty interesting challenge.
